{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"data_prepare.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"H6xm_JxIgZbz"},"source":["import numpy as np \n","import scipy as sp\n","import scipy.misc, scipy.ndimage.interpolation\n","import pandas as pd\n","import os\n","import nibabel as nib\n","import cv2\n","import matplotlib.pyplot as plt \n","from skimage import data, exposure\n","from glob import glob\n","import json\n","import warnings\n","warnings.filterwarnings('ignore', '.*output shape of zoom.*')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"APtH7Qg9gZb1"},"source":["# Find bounding box of 3d data\n","def bounding_box(image, expand_voxels = 10):\n","    rows = np.where(np.any(image, axis = (0, 2)))[0]\n","    cols = np.where(np.any(image, axis = (0, 1)))[0]\n","    depths = np.where(np.any(image, axis = (1, 2)))[0]\n","        \n","    if rows.size == 0 or cols.size == 0 or depths.size == 0:\n","        return -1, -1, -1, -1, -1, -1\n","    else:\n","        rmin, rmax = rows[[0, -1]]\n","        cmin, cmax = cols[[0, -1]]\n","        dmin, dmax = depths[[0, -1]]\n","            \n","        max_row = image.shape[1]\n","        max_col = image.shape[2]\n","        \n","        rmin = 0 if rmin - expand_voxels < 0 else rmin - expand_voxels\n","        rmax = max_row if rmax + expand_voxels >= max_row else rmax + expand_voxels + 1\n","        cmin = 0 if cmin - expand_voxels < 0 else cmin - expand_voxels\n","        cmax = max_col if cmax + expand_voxels >= max_col else cmax + expand_voxels + 1       \n","        \n","        return rmin, rmax, cmin, cmax, dmin, dmax + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZB75uMnfgZb2"},"source":["def create_train_json(image_paths, json_paths, json_save_path):\n","    #     categories = [{'id': 0, 'name': 'zh_str'}, {'id': 1, 'name': 'zh_ch'}, {'id': 2, 'name': 'en+num_str'}, \n","#                   {'id': 3, 'name': 'zh+en+num_str'}, {'id': 4, 'name': 'zh_str+ch'}, {'id': 5, 'name': 'other'}]\n","    categories = [{'id': 0, 'name': 'text'}]\n","    \n","    if os.path.isfile(json_save_path):\n","        with open(json_save_path, 'r') as f:\n","            json_file = json.load(f)\n","        annotations = json_file['annotations']\n","        image_infos = json_file['images']\n","    else:\n","        json_file = {}\n","        annotations = []\n","        image_infos = []\n","        \n","    annotation_id = len(annotations)\n","    image_id = len(image_infos)\n","    for index in range(len(image_paths)):\n","        # Load image\n","        img = cv2.imread(image_paths[index])\n","        # Create annotation for image\n","        image_info = {\n","            \"id\": image_id,\n","            \"file_name\": 'TrainDataset/img/' + os.path.split(image_paths[index])[-1],\n","            \"segm_file\": 'TrainDataset/img/' + 'gt_' + os.path.split(image_paths[index])[-1][:-3] + 'txt',\n","            \"width\": img.shape[1],\n","            \"height\": img.shape[0]\n","        }\n","        image_infos.append(image_info)\n","        #print(image_info)\n","        \n","        if json_paths is not None:\n","            with open(json_paths[index], 'r') as f:\n","                anns = json.load(f)\n","            \n","            for ann in anns['shapes']:\n","                points = np.array(ann['points']).astype('int32')\n","                area = cv2.contourArea(points)\n","                #category_id = ann['group_id']\n","                category_id = 0 if ann['group_id'] != 255 else 255\n","                \n","                annotation_info = {\n","                    \"id\": annotation_id,\n","                    \"image_id\": image_id,\n","                    \"category_id\": category_id,\n","                    \"iscrowd\": 0,\n","                    \"area\": area,\n","                    \"bbox\": [int(min(points[:,0])), int(min(points[:,1])), int(max(points[:,0]) - min(points[:,0])), int(max(points[:,1]) - min(points[:,1]))],\n","                    \"segmentation\": [points.flatten().tolist()]\n","                }\n","                #print(annotation_info)\n","                annotations.append(annotation_info)\n","                annotation_id += 1\n","        image_id += 1\n","        \n","    json_file.update({'images': image_infos, 'annotations': annotations, 'categories': categories})\n","    with open(json_save_path, 'w') as outfile:\n","        json.dump(json_file, outfile)\n","        \n","        \n","def create_test_json(image_paths, json_paths, json_save_path):\n","#     categories = [{'id': 0, 'name': 'zh_str'}, {'id': 1, 'name': 'zh_ch'}, {'id': 2, 'name': 'en+num_str'}, \n","#                   {'id': 3, 'name': 'zh+en+num_str'}, {'id': 4, 'name': 'zh_str+ch'}, {'id': 5, 'name': 'other'}]\n","    categories = [{'id': 0, 'name': 'text'}]\n","    \n","    if os.path.isfile(json_save_path):\n","        with open(json_save_path, 'r') as f:\n","            json_file = json.load(f)\n","        annotations = json_file['annotations']\n","        image_infos = json_file['images']\n","    else:\n","        json_file = {}\n","        annotations = []\n","        image_infos = []\n","        \n","    annotation_id = len(annotations)\n","    image_id = len(image_infos)\n","    for index in range(len(image_paths)):\n","        # Load image\n","        img = cv2.imread(image_paths[index])\n","        # Create annotation for image\n","        image_info = {\n","            \"id\": image_id,\n","            \"file_name\": 'PublicTestDataset/img/' + os.path.split(image_paths[index])[-1],\n","            \"segm_file\": 'PublicTestDataset/img/' + 'gt_' + os.path.split(image_paths[index])[-1][:-3] + 'txt',\n","            \"width\": img.shape[1],\n","            \"height\": img.shape[0]\n","        }\n","        image_infos.append(image_info)\n","        #print(image_info)\n","        \n","        if json_paths is not None:\n","            with open(json_paths[index], 'r') as f:\n","                anns = json.load(f)\n","            \n","            for ann in anns['shapes']:\n","                points = np.array(ann['points']).astype('int32')\n","                area = cv2.contourArea(points)\n","                #category_id = ann['group_id']\n","                category_id = 0 if ann['group_id'] != 255 else 255\n","                \n","                annotation_info = {\n","                    \"id\": annotation_id,\n","                    \"image_id\": image_id,\n","                    \"category_id\": category_id,\n","                    \"iscrowd\": 0,\n","                    \"area\": area,\n","                    \"bbox\": [int(min(points[:,0])), int(min(points[:,1])), int(max(points[:,0]) - min(points[:,0])), int(max(points[:,1]) - min(points[:,1]))],\n","                    \"segmentation\": [points.flatten().tolist()]\n","                }\n","                #print(annotation_info)\n","                annotations.append(annotation_info)\n","                annotation_id += 1\n","        image_id += 1\n","        \n","    json_file.update({'images': image_infos, 'annotations': annotations, 'categories': categories})\n","    with open(json_save_path, 'w') as outfile:\n","        json.dump(json_file, outfile)\n","        \n","        \n","def create_private_test_json(image_paths, json_paths, json_save_path):\n","#     categories = [{'id': 0, 'name': 'zh_str'}, {'id': 1, 'name': 'zh_ch'}, {'id': 2, 'name': 'en+num_str'}, \n","#                   {'id': 3, 'name': 'zh+en+num_str'}, {'id': 4, 'name': 'zh_str+ch'}, {'id': 5, 'name': 'other'}]\n","    categories = [{'id': 0, 'name': 'text'}]\n","    \n","    if os.path.isfile(json_save_path):\n","        with open(json_save_path, 'r') as f:\n","            json_file = json.load(f)\n","        annotations = json_file['annotations']\n","        image_infos = json_file['images']\n","    else:\n","        json_file = {}\n","        annotations = []\n","        image_infos = []\n","        \n","    annotation_id = len(annotations)\n","    image_id = len(image_infos)\n","    for index in range(len(image_paths)):\n","        # Load image\n","        img = cv2.imread(image_paths[index])\n","        # Create annotation for image\n","        image_info = {\n","            \"id\": image_id,\n","            \"file_name\": 'PrivateTestDataset/img/' + os.path.split(image_paths[index])[-1],\n","            \"segm_file\": 'PrivateTestDataset/img/' + 'gt_' + os.path.split(image_paths[index])[-1][:-3] + 'txt',\n","            \"width\": img.shape[1],\n","            \"height\": img.shape[0]\n","        }\n","        image_infos.append(image_info)\n","        #print(image_info)\n","        \n","        if json_paths is not None:\n","            with open(json_paths[index], 'r') as f:\n","                anns = json.load(f)\n","            \n","            for ann in anns['shapes']:\n","                points = np.array(ann['points']).astype('int32')\n","                area = cv2.contourArea(points)\n","                #category_id = ann['group_id']\n","                category_id = 0 if ann['group_id'] != 255 else 255\n","                \n","                annotation_info = {\n","                    \"id\": annotation_id,\n","                    \"image_id\": image_id,\n","                    \"category_id\": category_id,\n","                    \"iscrowd\": 0,\n","                    \"area\": area,\n","                    \"bbox\": [int(min(points[:,0])), int(min(points[:,1])), int(max(points[:,0]) - min(points[:,0])), int(max(points[:,1]) - min(points[:,1]))],\n","                    \"segmentation\": [points.flatten().tolist()]\n","                }\n","                #print(annotation_info)\n","                annotations.append(annotation_info)\n","                annotation_id += 1\n","        image_id += 1\n","        \n","    json_file.update({'images': image_infos, 'annotations': annotations, 'categories': categories})\n","    with open(json_save_path, 'w') as outfile:\n","        json.dump(json_file, outfile)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hl0haJZ-gZb4","outputId":"065e3f59-3b03-42c9-d7d6-780a9318e9c8"},"source":["path_for_train_data = './TrainDataset/img/'\n","path_for_train_json = './TrainDataset/json/'\n","path_for_test_data = './PublicTestDataset/img/'\n","path_for_private_test_data = './PrivateTestDataset/img/'\n","\n","train_prefix_len = len(path_for_train_data) + len('img_')\n","train_images = glob(os.path.join(path_for_train_data,'img_*'))\n","train_images = sorted(glob(os.path.join(path_for_train_data,'img_*')), key = lambda x: int(os.path.split(x)[-1][4:-4]))\n","print(len(train_images),' matching training image files found.')\n","\n","train_json_prefix_len = len(path_for_train_json) + len('img_')\n","train_jsons = glob(os.path.join(path_for_train_json,'img_*'))\n","train_jsons = sorted(glob(os.path.join(path_for_train_json,'img_*')), key = lambda x: int(os.path.split(x)[-1][4:-5]))\n","print(len(train_jsons),' matching training json files found.')\n","\n","test_prefix_len = len(path_for_test_data) + len('img_*')\n","test_images = glob(os.path.join(path_for_test_data,'img_*'))\n","test_images = sorted(glob(os.path.join(path_for_test_data,'img_*')), key = lambda x: int(os.path.split(x)[-1][4:-4]))\n","print(len(test_images),' matching testing image files found.')\n","\n","private_test_prefix_len = len(path_for_private_test_data) + len('img_*')\n","private_test_images = glob(os.path.join(path_for_private_test_data,'img_*'))\n","private_test_images = sorted(glob(os.path.join(path_for_private_test_data,'img_*')), key = lambda x: int(os.path.split(x)[-1][4:-4]))\n","print(len(private_test_images),' matching private testing image files found.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4000  matching training image files found.\n","4000  matching training json files found.\n","1000  matching testing image files found.\n","2500  matching private testing image files found.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4vrce42gZb5"},"source":["json_save_path = os.path.join('TrainDataset/', 'instances_training.json')\n","create_train_json(train_images, train_jsons, json_save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fd0XOSKQgZb5"},"source":["json_save_path = os.path.join('PublicTestDataset/', 'instances_public_test.json')\n","create_test_json(test_images, None, json_save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUUyrzb3gZb6"},"source":["json_save_path = os.path.join('PrivateTestDataset/', 'instances_private_test.json')\n","create_private_test_json(private_test_images, None, json_save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-pafc2DgZb6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yr-W2IqGgZb6"},"source":[""],"execution_count":null,"outputs":[]}]}